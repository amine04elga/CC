"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š GRAND GUIDE : ANALYSE DES TENDANCES DE MARCHÃ‰ & FACTEURS EXTERNES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Ce script gÃ©nÃ¨re un rapport complet d'analyse de donnÃ©es financiÃ¨res
dans le style pÃ©dagogique d'un Data Scientist expert.

Auteur : Analyse automatisÃ©e
Objectif : PrÃ©dire les tendances de marchÃ© avec XGBoost
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.metrics import (mean_squared_error, r2_score, mean_absolute_error,
                              accuracy_score, classification_report, confusion_matrix,
                              roc_auc_score, roc_curve)
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Configuration esthÃ©tique
sns.set_theme(style="whitegrid", palette="husl")
plt.rcParams['figure.figsize'] = (12, 6)
plt.rcParams['font.size'] = 10

print("="*70)
print("ğŸ“˜ ANATOMIE D'UN PROJET DATA SCIENCE : ANALYSE DE MARCHÃ‰")
print("="*70)
print()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PHASE 1 : CONTEXTE MÃ‰TIER ET MISSION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("â”€" * 70)
print("1ï¸âƒ£  CONTEXTE MÃ‰TIER : LA MISSION")
print("â”€" * 70)
print()
print("ğŸ“Œ LE PROBLÃˆME (BUSINESS CASE)")
print("   Dans le monde de la finance et du trading, les dÃ©cisions d'investissement")
print("   reposent sur la comprÃ©hension des tendances de marchÃ© et des facteurs")
print("   externes (Ã©conomiques, politiques, sociaux).")
print()
print("   ğŸ¯ Objectif : CrÃ©er un Assistant IA pour anticiper les mouvements de marchÃ©")
print("      en analysant des indicateurs externes (PIB, inflation, taux d'intÃ©rÃªt).")
print()
print("   âš ï¸  L'Enjeu Critique : La matrice des coÃ»ts d'erreur")
print("      â€¢ Faux Positif (prÃ©dire hausse â†’ investir â†’ baisse rÃ©elle)")
print("        â†’ Perte financiÃ¨re directe + coÃ»ts de transaction")
print("      â€¢ Faux NÃ©gatif (prÃ©dire baisse â†’ ne pas investir â†’ hausse rÃ©elle)")
print("        â†’ Manque Ã  gagner (opportunitÃ© ratÃ©e)")
print()
print("   ğŸ’¡ StratÃ©gie : Optimiser le ratio risque/rendement avec une IA robuste")
print("      qui minimise les fausses alertes tout en capturant les vraies opportunitÃ©s.")
print()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PHASE 2 : ACQUISITION DES DONNÃ‰ES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("â”€" * 70)
print("2ï¸âƒ£  ACQUISITION & CHARGEMENT DES DONNÃ‰ES")
print("â”€" * 70)
print()

try:
    import kagglehub
    print("ğŸ“¥ TÃ©lÃ©chargement du dataset depuis Kaggle...")
    path = kagglehub.dataset_download("kundanbedmutha/market-trend-and-external-factors-dataset")
    print(f"âœ… Dataset tÃ©lÃ©chargÃ© dans : {path}")
    
    # Recherche du fichier CSV
    import os
    csv_files = [f for f in os.listdir(path) if f.endswith('.csv')]
    
    if csv_files:
        df = pd.read_csv(os.path.join(path, csv_files[0]))
        print(f"âœ… Fichier chargÃ© : {csv_files[0]}")
    else:
        raise FileNotFoundError("Aucun fichier CSV trouvÃ©")
        
except Exception as e:
    print(f"âš ï¸  Erreur lors du tÃ©lÃ©chargement : {e}")
    print("ğŸ“ GÃ©nÃ©ration de donnÃ©es synthÃ©tiques pour dÃ©monstration...")
    
    # CrÃ©ation de donnÃ©es synthÃ©tiques rÃ©alistes
    np.random.seed(42)
    n_samples = 1000
    
    # Simulation de facteurs Ã©conomiques
    dates = pd.date_range('2020-01-01', periods=n_samples, freq='D')
    market_base = 1000
    market_trend = np.cumsum(np.random.randn(n_samples) * 2 + 0.05)
    
    df = pd.DataFrame({
        'Date': dates,
        'Market_Index': market_base + market_trend,
        'GDP_Growth': np.random.uniform(1.5, 4.5, n_samples) + np.sin(np.arange(n_samples)/50) * 0.5,
        'Inflation_Rate': np.random.uniform(1.0, 5.0, n_samples) + np.cos(np.arange(n_samples)/60) * 0.3,
        'Interest_Rate': np.random.uniform(0.5, 3.5, n_samples),
        'Unemployment_Rate': np.random.uniform(3.0, 8.0, n_samples),
        'Consumer_Confidence': np.random.uniform(80, 120, n_samples),
        'Oil_Price': np.random.uniform(40, 100, n_samples) + np.sin(np.arange(n_samples)/30) * 10,
        'Gold_Price': np.random.uniform(1500, 2000, n_samples),
        'USD_Exchange_Rate': np.random.uniform(0.85, 1.15, n_samples),
        'Market_Volatility': np.random.uniform(10, 40, n_samples),
        'Trade_Volume': np.random.uniform(1e6, 1e7, n_samples),
    })
    
    # CrÃ©ation d'une variable cible : Tendance du marchÃ© (1=Hausse, 0=Baisse)
    df['Price_Change'] = df['Market_Index'].pct_change() * 100
    df['Market_Trend'] = (df['Price_Change'] > 0).astype(int)
    df.loc[0, 'Market_Trend'] = 1  # PremiÃ¨re valeur
    df.loc[0, 'Price_Change'] = 0

print()
print(f"ğŸ“Š Dimensions du dataset : {df.shape[0]} lignes Ã— {df.shape[1]} colonnes")
print()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PHASE 3 : EXPLORATION INITIALE (FIRST LOOK)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("â”€" * 70)
print("3ï¸âƒ£  EXPLORATION INITIALE : PREMIER REGARD SUR LES DONNÃ‰ES")
print("â”€" * 70)
print()
print("ğŸ“‹ AperÃ§u des premiÃ¨res lignes :")
print(df.head())
print()
print("ğŸ” Informations sur les types de donnÃ©es :")
print(df.info())
print()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PHASE 4 : DATA WRANGLING (NETTOYAGE)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("â”€" * 70)
print("4ï¸âƒ£  DATA WRANGLING : NETTOYAGE ET PRÃ‰PARATION")
print("â”€" * 70)
print()

# Simulation de donnÃ©es manquantes (rÃ©alisme)
df_dirty = df.copy()
numeric_cols = df.select_dtypes(include=[np.number]).columns
exclude_target = [col for col in numeric_cols if 'trend' not in col.lower() and 'target' not in col.lower()]

for col in exclude_target[:6]:  # Corrompre 6 colonnes
    mask = np.random.rand(len(df)) < 0.04  # 4% de valeurs manquantes
    df_dirty.loc[mask, col] = np.nan

print(f"âš ï¸  Valeurs manquantes introduites (simulation de la rÃ©alitÃ©) :")
missing = df_dirty.isnull().sum()
missing_display = missing[missing > 0]
if len(missing_display) > 0:
    print(missing_display)
else:
    print("   Aucune valeur manquante dans ce dataset")
print()

# SÃ©paration features/target
# Identifier automatiquement la variable cible
if 'Market_Trend' in df_dirty.columns:
    target_col = 'Market_Trend'
    problem_type = 'classification'
elif 'Trend' in df_dirty.columns:
    target_col = 'Trend'
    problem_type = 'classification'
elif 'Market_Index' in df_dirty.columns:
    target_col = 'Market_Index'
    problem_type = 'regression'
else:
    # Prendre la derniÃ¨re colonne numÃ©rique
    numeric_cols = df_dirty.select_dtypes(include=[np.number]).columns
    target_col = numeric_cols[-1]
    problem_type = 'regression'

# Exclure les colonnes de date et variables dÃ©rivÃ©es
date_cols = df_dirty.select_dtypes(include=['datetime64', 'object']).columns
exclude_cols = list(date_cols) + [target_col]
if 'Price_Change' in df_dirty.columns and target_col != 'Price_Change':
    exclude_cols.append('Price_Change')

X = df_dirty.drop(columns=exclude_cols, errors='ignore')
y = df_dirty[target_col]

print(f"ğŸ¯ Variable cible identifiÃ©e : {target_col}")
print(f"ğŸ“Š Type de problÃ¨me : {problem_type.upper()}")
print(f"ğŸ“ Features sÃ©lectionnÃ©es : {X.shape[1]} variables")
print(f"   â†’ {list(X.columns)[:10]}")  # Afficher les 10 premiÃ¨res
if len(X.columns) > 10:
    print(f"   ... et {len(X.columns) - 10} autres")
print()

# Imputation des valeurs manquantes
print("ğŸ”§ StratÃ©gie d'imputation : Moyenne (mean)")
print("   MÃ©canisme de SimpleImputer :")
print("   â”Œâ”€ fit() : Calcul Î¼ (moyenne) sur chaque colonne des donnÃ©es disponibles")
print("   â”‚         Ex: Colonne 'GDP_Growth' â†’ Î¼ = 2.85%")
print("   â””â”€ transform() : Pour chaque NaN dÃ©tectÃ© â†’ injection de Î¼")
print()

imputer = SimpleImputer(strategy='mean')
X_imputed = imputer.fit_transform(X)
X_clean = pd.DataFrame(X_imputed, columns=X.columns)

print(f"âœ… Nettoyage terminÃ© : {X_clean.isnull().sum().sum()} valeurs manquantes restantes")
print()

# âš ï¸ AVERTISSEMENT DATA LEAKAGE
print("ğŸ’¡ COIN DE L'EXPERT : Data Leakage")
print("   âš ï¸  Dans ce script pÃ©dagogique, nous avons imputÃ© AVANT de sÃ©parer Train/Test.")
print("   En production rigoureuse, c'est une ERREUR subtile :")
print()
print("   âŒ Mauvaise pratique (ici) :")
print("      1. fit(toutes_les_donnÃ©es)  â† La moyenne inclut le Test Set")
print("      2. transform(toutes_les_donnÃ©es)")
print("      3. train_test_split()")
print()
print("   âœ… Bonne pratique industrielle :")
print("      1. train_test_split()  â† SÃ©parer d'abord")
print("      2. fit(X_train)  â† Calculer Î¼ uniquement sur Train")
print("      3. transform(X_train) et transform(X_test)  â† Appliquer le mÃªme Î¼")
print()
print("   ğŸ’° Impact financier : Le Data Leakage peut entraÃ®ner un sur-optimisme")
print("      de +2 Ã  +5% en accuracy, ce qui fausse les dÃ©cisions d'investissement.")
print()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PHASE 5 : ANALYSE EXPLORATOIRE (EDA)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("â”€" * 70)
print("5ï¸âƒ£  ANALYSE EXPLORATOIRE : PROFILAGE DES DONNÃ‰ES")
print("â”€" * 70)
print()

print("ğŸ“Š Statistiques descriptives (5 premiÃ¨res features) :")
print(X_clean.iloc[:, :5].describe().round(2))
print()

print("ğŸ” DÃ‰CRYPTAGE DE .describe() :")
print("   â€¢ count : Nombre d'observations (devrait Ãªtre constant aprÃ¨s imputation)")
print("   â€¢ mean vs 50% (mÃ©diane) :")
print("     â†’ Si mean >> mÃ©diane : Distribution asymÃ©trique (skewed right)")
print("     â†’ Exemple : Si mean=3.2 et 50%=2.8 pour GDP â†’ Pics exceptionnels tirÃ©s vers le haut")
print("   â€¢ std (Ã‰cart-type) : Mesure de volatilitÃ©")
print("     â†’ std Ã©levÃ© = variable instable (ex: Oil_Price)")
print("     â†’ std â‰ˆ 0 = variable quasi-constante (inutile pour le ML)")
print("   â€¢ min/max : DÃ©tection d'outliers potentiels")
print("     â†’ Exemple : Interest_Rate Ã  15% serait suspect")
print()

# Matrice de corrÃ©lation
print("ğŸŒ¡ï¸  Analyse de la multicollinÃ©aritÃ©...")
corr_matrix = X_clean.corr()
high_corr = np.where(np.abs(corr_matrix) > 0.9)
high_corr_pairs = [(corr_matrix.index[x], corr_matrix.columns[y], corr_matrix.iloc[x, y]) 
                   for x, y in zip(*high_corr) if x != y and x < y]

if high_corr_pairs:
    print("âš ï¸  Variables fortement corrÃ©lÃ©es dÃ©tectÃ©es (|r| > 0.9) :")
    for var1, var2, corr in high_corr_pairs[:3]:
        print(f"   â€¢ {var1} â†” {var2} : {corr:.3f}")
    print()
    print("   ğŸ’¡ Le problÃ¨me de la redondance :")
    print("      GÃ©omÃ©triquement : Deux variables corrÃ©lÃ©es portent la mÃªme information")
    print("      Impact ML :")
    print("      â†’ XGBoost : TOLÃ‰RANT (robuste Ã  la multicollinÃ©aritÃ©)")
    print("      â†’ RÃ©gression LinÃ©aire : PROBLÃ‰MATIQUE (coefficients instables)")
else:
    print("âœ… Pas de multicollinÃ©aritÃ© excessive dÃ©tectÃ©e")
print()

# Analyse de la distribution de la cible
print("ğŸ¯ Analyse de la variable cible :")
if problem_type == 'classification':
    print(y.value_counts())
    balance_ratio = y.value_counts().min() / y.value_counts().max()
    print(f"   Balance ratio : {balance_ratio:.2f}")
    if balance_ratio < 0.5:
        print("   âš ï¸  Classes dÃ©sÃ©quilibrÃ©es dÃ©tectÃ©es ! ConsidÃ©rer SMOTE ou class_weight")
    else:
        print("   âœ… Classes relativement Ã©quilibrÃ©es")
else:
    print(f"   Moyenne : {y.mean():.2f}")
    print(f"   Std : {y.std():.2f}")
    print(f"   Min : {y.min():.2f} | Max : {y.max():.2f}")
print()

# Visualisation
fig = plt.figure(figsize=(14, 5))

# Distribution de la cible
ax1 = plt.subplot(1, 3, 1)
if problem_type == 'classification':
    counts = y.value_counts()
    ax1.bar(counts.index, counts.values, color=['#FF6B6B', '#4ECDC4'], edgecolor='black')
    ax1.set_title('Distribution de la Variable Cible', fontsize=12, fontweight='bold')
    ax1.set_xlabel(target_col)
    ax1.set_ylabel('FrÃ©quence')
    ax1.set_xticks(counts.index)
else:
    ax1.hist(y, bins=50, color='#4ECDC4', edgecolor='black', alpha=0.7)
    ax1.set_title('Distribution de la Variable Cible', fontsize=12, fontweight='bold')
    ax1.set_xlabel(target_col)
    ax1.set_ylabel('FrÃ©quence')

# Heatmap de corrÃ©lation (top 8 features)
ax2 = plt.subplot(1, 3, 2)
top_features = X_clean.columns[:min(8, len(X_clean.columns))]
sns.heatmap(X_clean[top_features].corr(), annot=True, fmt='.2f', 
            cmap='coolwarm', center=0, square=True, linewidths=1, ax=ax2)
ax2.set_title('Matrice de CorrÃ©lation (Top Features)', fontsize=12, fontweight='bold')

# Boxplot pour dÃ©tecter les outliers
ax3 = plt.subplot(1, 3, 3)
sample_features = X_clean.iloc[:, :4]
sample_features_scaled = (sample_features - sample_features.mean()) / sample_features.std()
ax3.boxplot([sample_features_scaled.iloc[:, i] for i in range(sample_features_scaled.shape[1])],
            labels=sample_features_scaled.columns)
ax3.set_title('DÃ©tection d\'Outliers (StandardisÃ©)', fontsize=12, fontweight='bold')
ax3.set_ylabel('Valeur (Z-score)')
ax3.axhline(y=0, color='r', linestyle='--', alpha=0.5)
plt.xticks(rotation=45, ha='right')

plt.tight_layout()
plt.savefig('eda_analysis.png', dpi=150, bbox_inches='tight')
print("ğŸ“ˆ Graphiques sauvegardÃ©s : eda_analysis.png")
print()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PHASE 6 : PROTOCOLE EXPÃ‰RIMENTAL (SPLIT)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("â”€" * 70)
print("6ï¸âƒ£  PROTOCOLE EXPÃ‰RIMENTAL : TRAIN/TEST SPLIT")
print("â”€" * 70)
print()

X_train, X_test, y_train, y_test = train_test_split(
    X_clean, y, test_size=0.2, random_state=42, stratify=y if problem_type == 'classification' else None
)

print("ğŸ“ Le Concept : La Garantie de GÃ©nÃ©ralisation")
print("   Le but du ML n'est PAS de MÃ‰MORISER le passÃ© (overfitting),")
print("   mais de GÃ‰NÃ‰RALISER vers le futur (prÃ©dictions robustes).")
print()
print("   ğŸ§ª Analogie : Un Ã©tudiant qui mÃ©morise les rÃ©ponses d'un examen")
print("      vs un Ã©tudiant qui comprend les concepts.")
print()
print(f"âœ‚ï¸  SÃ©paration effectuÃ©e :")
print(f"   â€¢ Train Set : {X_train.shape[0]} Ã©chantillons (80%) â†’ Apprentissage")
print(f"   â€¢ Test Set  : {X_test.shape[0]} Ã©chantillons (20%) â†’ Ã‰valuation")
if problem_type == 'classification':
    print(f"   â€¢ Stratification activÃ©e : maintient les proportions de classes")
print()
print("ğŸ” Les ParamÃ¨tres sous le Capot :")
print("   â€¢ test_size=0.2 : Le ratio 80/20 (Principe de Pareto)")
print("     â†’ 80% pour capturer la complexitÃ© des patterns")
print("     â†’ 20% pour une Ã©valuation statistiquement significative")
print("   â€¢ random_state=42 : ReproductibilitÃ© scientifique")
print("     â†’ Deux exÃ©cutions = rÃ©sultats identiques")
print("     â†’ Partage avec un collÃ¨gue au Japon = mÃªmes patients dans Test Set")
print()

# Standardisation (critique pour certains algos, optionnelle pour XGBoost)
print("âš–ï¸  Standardisation des features...")
print("   Note : XGBoost est invariant Ã  l'Ã©chelle (tree-based),")
print("   mais on standardise pour cohÃ©rence avec d'autres algos potentiels.")
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
print("   âœ… Transformation : mean=0, std=1 pour chaque feature")
print()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PHASE 7 : MODÃ‰LISATION (XGBOOST)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("â”€" * 70)
print("7ï¸âƒ£  INTELLIGENCE ARTIFICIELLE : XGBOOST ğŸš€")
print("â”€" * 70)
print()

print("ğŸ§  POURQUOI XGBOOST POUR LES MARCHÃ‰S FINANCIERS ?")
print()
print("A. Le Champion des CompÃ©titions Kaggle")
print("   XGBoost (eXtreme Gradient Boosting) domine les compÃ©titions de Data Science")
print("   depuis 2014. Il a rÃ©volutionnÃ© le ML grÃ¢ce Ã  :")
print("   â€¢ Vitesse d'exÃ©cution : 10x plus rapide que les autres Gradient Boosting")
print("   â€¢ Gestion native des valeurs manquantes")
print("   â€¢ RÃ©gularisation intÃ©grÃ©e (L1/L2) â†’ RÃ©duit l'overfitting")
print()
print("B. La MÃ©canique : Boosting SÃ©quentiel")
print("   Contrairement au Random Forest (Bagging = arbres en parallÃ¨le),")
print("   XGBoost utilise le Boosting = arbres en SÃ‰QUENCE.")
print()
print("   Ã‰tape 1 : L'Arbre #1 fait des prÃ©dictions")
print("            â†’ Il se trompe sur certains points")
print()
print("   Ã‰tape 2 : L'Arbre #2 se concentre sur ces erreurs")
print("            â†’ Il apprend Ã  corriger les faiblesses de l'Arbre #1")
print()
print("   Ã‰tape 3 : L'Arbre #3 corrige les erreurs restantes...")
print()
print("   Ã‰tape 100 : 100 arbres collaborent de maniÃ¨re sÃ©quentielle")
print()
print("   ğŸ’¡ Formule mathÃ©matique :")
print("      Å·_final = Å·_arbre1 + Î·Â·Å·_arbre2 + Î·Â·Å·_arbre3 + ... + Î·Â·Å·_arbre100")
print("      oÃ¹ Î· (learning_rate) contrÃ´le la contribution de chaque arbre")
print()
print("C. Les HyperparamÃ¨tres Critiques")
print("   â€¢ n_estimators : Nombre d'arbres (â†‘ prÃ©cision, â†“ risque overfitting)")
print("   â€¢ max_depth : Profondeur max (â†‘ complexitÃ©, â†‘ risque overfitting)")
print("   â€¢ learning_rate : Pas d'apprentissage (petit = robuste mais lent)")
print("   â€¢ subsample : % de donnÃ©es par arbre (ajoute randomness)")
print()

try:
    import xgboost as xgb
    xgboost_available = True
except ImportError:
    xgboost_available = False
    print("âš ï¸  XGBoost n'est pas installÃ©. Installation : pip install xgboost")
    print("ğŸ“¦ Utilisation de GradientBoosting (sklearn) comme alternative...")
    from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor

if xgboost_available:
    if problem_type == 'classification':
        model = xgb.XGBClassifier(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            eval_metric='logloss'
        )
    else:
        model = xgb.XGBRegressor(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42
        )
    model_name = "XGBoost"
else:
    if problem_type == 'classification':
        model = GradientBoostingClassifier(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            subsample=0.8,
            random_state=42
        )
    else:
        model = GradientBoostingRegressor(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            subsample=0.8,
            random_state=42
        )
    model_name = "GradientBoosting"

print(f"ğŸ—ï¸  Construction du modÃ¨le {model_name}...")
print(f"   â€¢ n_estimators=100 (100 arbres sÃ©quentiels)")
print(f"   â€¢ max_depth=6 (profondeur limitÃ©e pour Ã©viter l'overfitting)")
print(f"   â€¢ learning_rate=0.1 (contribution modÃ©rÃ©e de chaque arbre)")
print(f"   â€¢ subsample=0.8 (80% des donnÃ©es par arbre â†’ diversitÃ©)")
print()

print("ğŸš€ EntraÃ®nement en cours...")
print("   Phase 1 : Construction de l'arbre #1...")
model.fit(X_train_scaled, y_train)
print(f"   Phase 100 : AgrÃ©gation des 100 arbres â†’ PrÃ©diction finale")
print(f"âœ… ModÃ¨le {model_name} entraÃ®nÃ© avec succÃ¨s !")
print()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PHASE 8 : Ã‰VALUATION (L'HEURE DE VÃ‰RITÃ‰)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("â”€" * 70)
print("8ï¸âƒ£  AUDIT DE PERFORMANCE : L'HEURE DE VÃ‰RITÃ‰")
print("â”€" * 70)
print()

y_pred = model.predict(X_test_scaled)

if problem_type == 'classification':
    accuracy = accuracy_score(y_test, y_pred)
    print(f"ğŸ¯ ACCURACY GLOBALE : {accuracy*100:.2f}%")
    print()
    
    # PrÃ©dictions probabilistes pour ROC-AUC
    if hasattr(model, 'predict_proba'):
        y_pred_proba = model.predict_proba(X_test_scaled)[:, 1]
        roc_auc = roc_auc_score(y_test, y_pred_proba)
        print(f"ğŸ“Š ROC-AUC SCORE : {roc_auc:.4f}")
        print("   â†’ 0.5 = hasard | 1.0 = parfait | >0.75 = bon modÃ¨le")
        print()
    
    print("ğŸ“‹ RAPPORT DÃ‰TAILLÃ‰ (Classification Report) :")
    print(classification_report(y_test, y_pred, digits=3))
    print()
    
    print("ğŸ” DÃ‰CRYPTAGE DES MÃ‰TRIQUES (Application Finance) :")
    print()
    print("   1. PRECISION (QualitÃ© de l'alarme)")
    print("      Formule : TP / (TP + FP)")
    print("      Question : Quand l'IA prÃ©dit 'Hausse', est-ce fiable ?")
    print("      â†’ Precision basse = Beaucoup de fausses alertes")
    print("      ğŸ’° Impact : CoÃ»ts de transaction inutiles")
    print()
    print("   2. RECALL / SENSIBILITÃ‰ (Puissance du filet)")
    print("      Formule : TP / (TP + FN)")
    print("      Question : Sur toutes les vraies hausses, combien sont dÃ©tectÃ©es ?")
    print("      â†’ Recall bas = OpportunitÃ©s ratÃ©es")
    print("      ğŸ’° Impact : Manque Ã  gagner")
    print()
    print("   3. F1-SCORE (Ã‰quilibre)")
    print("      Formule : 2 Ã— (Precision Ã— Recall) / (Precision + Recall)")
    print("      â†’ Moyenne harmonique (pÃ©nalise les dÃ©sÃ©quilibres)")
    print("      â†’ F1 Ã©levÃ© = Bon compromis entre qualitÃ© et couverture")
    print()
    
    # Matrice de confusion
    cm = confusion_matrix(y_test, y_pred)
    
    fig = plt.figure(figsize=(14, 5))
    
    # Matrice de confusion
    ax1 = plt.subplot(1, 3, 1)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar_kws={'label': 'Nombre'}, ax=ax1)
    ax1.set_title('Matrice de Confusion : RÃ©alitÃ© vs IA', fontsize=12, fontweight='bold')
    ax1.set_ylabel('Vraie Classe', fontsize=11)
    ax1.set_xlabel('Classe PrÃ©dite', font
