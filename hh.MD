"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
ğŸ“Š GRAND GUIDE : ANALYSE DES TENDANCES DE MARCHÃ‰ & FACTEURS EXTERNES
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

Ce script gÃ©nÃ¨re un rapport complet d'analyse de donnÃ©es financiÃ¨res
dans le style pÃ©dagogique d'un Data Scientist expert.

Auteur : Analyse automatisÃ©e
Objectif : PrÃ©dire les tendances de marchÃ© en fonction de facteurs externes
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.metrics import (mean_squared_error, r2_score, mean_absolute_error,
                              accuracy_score, classification_report, confusion_matrix)
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Configuration esthÃ©tique
sns.set_theme(style="whitegrid", palette="husl")
plt.rcParams['figure.figsize'] = (12, 6)
plt.rcParams['font.size'] = 10

print("="*70)
print("ğŸ“˜ ANATOMIE D'UN PROJET DATA SCIENCE : ANALYSE DE MARCHÃ‰")
print("="*70)
print()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PHASE 1 : CONTEXTE MÃ‰TIER ET MISSION
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("â”€" * 70)
print("1ï¸âƒ£  CONTEXTE MÃ‰TIER : LA MISSION")
print("â”€" * 70)
print()
print("ğŸ“Œ LE PROBLÃˆME (BUSINESS CASE)")
print("   Dans le monde de la finance et du trading, les dÃ©cisions d'investissement")
print("   reposent sur la comprÃ©hension des tendances de marchÃ© et des facteurs")
print("   externes (Ã©conomiques, politiques, sociaux).")
print()
print("   ğŸ¯ Objectif : CrÃ©er un modÃ¨le prÃ©dictif pour anticiper les mouvements")
print("      de marchÃ© en analysant des indicateurs externes.")
print()
print("   âš ï¸  L'Enjeu Critique : ")
print("      â€¢ Faux Positif (prÃ©dire une hausse qui n'arrive pas) â†’ Perte financiÃ¨re")
print("      â€¢ Faux NÃ©gatif (manquer une opportunitÃ©) â†’ Manque Ã  gagner")
print("      â†’ L'IA doit optimiser le ratio risque/rendement")
print()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PHASE 2 : ACQUISITION DES DONNÃ‰ES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("â”€" * 70)
print("2ï¸âƒ£  ACQUISITION & CHARGEMENT DES DONNÃ‰ES")
print("â”€" * 70)
print()

try:
    import kagglehub
    print("ğŸ“¥ TÃ©lÃ©chargement du dataset depuis Kaggle...")
    path = kagglehub.dataset_download("kundanbedmutha/market-trend-and-external-factors-dataset")
    print(f"âœ… Dataset tÃ©lÃ©chargÃ© dans : {path}")
    
    # Recherche du fichier CSV
    import os
    csv_files = [f for f in os.listdir(path) if f.endswith('.csv')]
    
    if csv_files:
        df = pd.read_csv(os.path.join(path, csv_files[0]))
        print(f"âœ… Fichier chargÃ© : {csv_files[0]}")
    else:
        raise FileNotFoundError("Aucun fichier CSV trouvÃ©")
        
except Exception as e:
    print(f"âš ï¸  Erreur lors du tÃ©lÃ©chargement : {e}")
    print("ğŸ“ GÃ©nÃ©ration de donnÃ©es synthÃ©tiques pour dÃ©monstration...")
    
    # CrÃ©ation de donnÃ©es synthÃ©tiques rÃ©alistes
    np.random.seed(42)
    n_samples = 1000
    
    df = pd.DataFrame({
        'Date': pd.date_range('2020-01-01', periods=n_samples, freq='D'),
        'Market_Index': np.cumsum(np.random.randn(n_samples) * 2 + 0.05) + 1000,
        'GDP_Growth': np.random.uniform(1.5, 4.5, n_samples),
        'Inflation_Rate': np.random.uniform(1.0, 5.0, n_samples),
        'Interest_Rate': np.random.uniform(0.5, 3.5, n_samples),
        'Unemployment_Rate': np.random.uniform(3.0, 8.0, n_samples),
        'Consumer_Confidence': np.random.uniform(80, 120, n_samples),
        'Oil_Price': np.random.uniform(40, 100, n_samples),
        'Gold_Price': np.random.uniform(1500, 2000, n_samples),
        'USD_Exchange_Rate': np.random.uniform(0.85, 1.15, n_samples),
        'Market_Volatility': np.random.uniform(10, 40, n_samples),
    })
    
    # CrÃ©ation d'une variable cible : Tendance du marchÃ© (1=Hausse, 0=Baisse)
    df['Market_Trend'] = (df['Market_Index'].pct_change() > 0).astype(int)
    df.loc[0, 'Market_Trend'] = 1  # PremiÃ¨re valeur

print()
print(f"ğŸ“Š Dimensions du dataset : {df.shape[0]} lignes Ã— {df.shape[1]} colonnes")
print()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PHASE 3 : EXPLORATION INITIALE (FIRST LOOK)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("â”€" * 70)
print("3ï¸âƒ£  EXPLORATION INITIALE : PREMIER REGARD SUR LES DONNÃ‰ES")
print("â”€" * 70)
print()
print("ğŸ“‹ AperÃ§u des premiÃ¨res lignes :")
print(df.head())
print()
print("ğŸ” Informations sur les types de donnÃ©es :")
print(df.info())
print()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PHASE 4 : DATA WRANGLING (NETTOYAGE)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("â”€" * 70)
print("4ï¸âƒ£  DATA WRANGLING : NETTOYAGE ET PRÃ‰PARATION")
print("â”€" * 70)
print()

# Simulation de donnÃ©es manquantes (rÃ©alisme)
df_dirty = df.copy()
numeric_cols = df.select_dtypes(include=[np.number]).columns
for col in numeric_cols[:5]:  # Corrompre 5 colonnes
    mask = np.random.rand(len(df)) < 0.03  # 3% de valeurs manquantes
    df_dirty.loc[mask, col] = np.nan

print(f"âš ï¸  Valeurs manquantes introduites (simulation de la rÃ©alitÃ©) :")
missing = df_dirty.isnull().sum()
print(missing[missing > 0])
print()

# SÃ©paration features/target
# Identifier automatiquement la variable cible
if 'Market_Trend' in df_dirty.columns:
    target_col = 'Market_Trend'
    problem_type = 'classification'
elif 'Market_Index' in df_dirty.columns:
    target_col = 'Market_Index'
    problem_type = 'regression'
else:
    # Prendre la derniÃ¨re colonne numÃ©rique
    numeric_cols = df_dirty.select_dtypes(include=[np.number]).columns
    target_col = numeric_cols[-1]
    problem_type = 'regression'

# Exclure les colonnes de date
date_cols = df_dirty.select_dtypes(include=['datetime64', 'object']).columns
X = df_dirty.drop(columns=[target_col] + list(date_cols))
y = df_dirty[target_col]

print(f"ğŸ¯ Variable cible identifiÃ©e : {target_col}")
print(f"ğŸ“Š Type de problÃ¨me : {problem_type.upper()}")
print(f"ğŸ“ Features sÃ©lectionnÃ©es : {X.shape[1]} variables")
print(f"   â†’ {list(X.columns)}")
print()

# Imputation des valeurs manquantes
print("ğŸ”§ StratÃ©gie d'imputation : Moyenne (mean)")
print("   â”Œâ”€ fit() : Calcul de la moyenne sur les donnÃ©es disponibles")
print("   â””â”€ transform() : Remplissage des trous avec cette moyenne")
print()

imputer = SimpleImputer(strategy='mean')
X_imputed = imputer.fit_transform(X)
X_clean = pd.DataFrame(X_imputed, columns=X.columns)

print(f"âœ… Nettoyage terminÃ© : 0 valeurs manquantes restantes")
print()

# âš ï¸ AVERTISSEMENT DATA LEAKAGE
print("ğŸ’¡ COIN DE L'EXPERT : Data Leakage")
print("   Dans ce script pÃ©dagogique, nous avons imputÃ© AVANT de sÃ©parer Train/Test.")
print("   En production, c'est une ERREUR subtile :")
print("   â†’ La moyenne calculÃ©e inclut des informations du Test Set")
print("   â†’ Risque de sur-optimisme dans les performances")
print()
print("   âœ“ Bonne pratique : fit() sur Train uniquement, transform() sur Train ET Test")
print()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PHASE 5 : ANALYSE EXPLORATOIRE (EDA)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("â”€" * 70)
print("5ï¸âƒ£  ANALYSE EXPLORATOIRE : PROFILAGE DES DONNÃ‰ES")
print("â”€" * 70)
print()

print("ğŸ“Š Statistiques descriptives (5 premiÃ¨res features) :")
print(X_clean.iloc[:, :5].describe().round(2))
print()

print("ğŸ” DÃ‰CRYPTAGE DE .describe() :")
print("   â€¢ Mean vs 50% (MÃ©diane) : Si Mean >> MÃ©diane â†’ Distribution asymÃ©trique")
print("   â€¢ Std (Ã‰cart-type) : Mesure de dispersion (std â‰ˆ 0 â†’ variable inutile)")
print("   â€¢ Min/Max : DÃ©tection des valeurs aberrantes potentielles")
print()

# Matrice de corrÃ©lation
print("ğŸŒ¡ï¸  Analyse de la multicollinÃ©aritÃ©...")
corr_matrix = X_clean.corr()
high_corr = np.where(np.abs(corr_matrix) > 0.9)
high_corr_pairs = [(corr_matrix.index[x], corr_matrix.columns[y], corr_matrix.iloc[x, y]) 
                   for x, y in zip(*high_corr) if x != y and x < y]

if high_corr_pairs:
    print("âš ï¸  Variables fortement corrÃ©lÃ©es dÃ©tectÃ©es (>0.9) :")
    for var1, var2, corr in high_corr_pairs[:3]:
        print(f"   â€¢ {var1} â†” {var2} : {corr:.3f}")
    print()
    print("   ğŸ’¡ Impact : Redondance d'information (acceptable pour Random Forest)")
else:
    print("âœ… Pas de multicollinÃ©aritÃ© excessive dÃ©tectÃ©e")
print()

# Visualisation de la distribution de la cible
plt.figure(figsize=(10, 4))
if problem_type == 'classification':
    plt.subplot(1, 2, 1)
    y.value_counts().plot(kind='bar', color=['#FF6B6B', '#4ECDC4'])
    plt.title('Distribution de la Variable Cible')
    plt.xlabel(target_col)
    plt.ylabel('FrÃ©quence')
    plt.xticks(rotation=0)
else:
    plt.subplot(1, 2, 1)
    plt.hist(y, bins=50, color='#4ECDC4', edgecolor='black', alpha=0.7)
    plt.title('Distribution de la Variable Cible')
    plt.xlabel(target_col)
    plt.ylabel('FrÃ©quence')

# Heatmap de corrÃ©lation (top 8 features)
plt.subplot(1, 2, 2)
top_features = X_clean.columns[:8]
sns.heatmap(X_clean[top_features].corr(), annot=True, fmt='.2f', 
            cmap='coolwarm', center=0, square=True, linewidths=1)
plt.title('Matrice de CorrÃ©lation (8 premiÃ¨res features)')
plt.tight_layout()
plt.savefig('eda_analysis.png', dpi=150, bbox_inches='tight')
print("ğŸ“ˆ Graphiques sauvegardÃ©s : eda_analysis.png")
print()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PHASE 6 : PROTOCOLE EXPÃ‰RIMENTAL (SPLIT)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("â”€" * 70)
print("6ï¸âƒ£  PROTOCOLE EXPÃ‰RIMENTAL : TRAIN/TEST SPLIT")
print("â”€" * 70)
print()

X_train, X_test, y_train, y_test = train_test_split(
    X_clean, y, test_size=0.2, random_state=42
)

print("ğŸ“ Principe : La Garantie de GÃ©nÃ©ralisation")
print("   Le but du ML n'est pas de MÃ‰MORISER le passÃ©, mais de GÃ‰NÃ‰RALISER au futur.")
print()
print(f"âœ‚ï¸  SÃ©paration effectuÃ©e :")
print(f"   â€¢ Train Set : {X_train.shape[0]} Ã©chantillons (80%) â†’ Apprentissage")
print(f"   â€¢ Test Set  : {X_test.shape[0]} Ã©chantillons (20%) â†’ Ã‰valuation")
print()
print("ğŸ” random_state=42 â†’ ReproductibilitÃ© scientifique garantie")
print("   (Deux exÃ©cutions = rÃ©sultats identiques)")
print()

# Standardisation (optionnelle mais recommandÃ©e)
print("âš–ï¸  Standardisation des features (mean=0, std=1)...")
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)
print("âœ… Mise Ã  l'Ã©chelle terminÃ©e")
print()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PHASE 7 : MODÃ‰LISATION (RANDOM FOREST)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("â”€" * 70)
print("7ï¸âƒ£  INTELLIGENCE ARTIFICIELLE : RANDOM FOREST ğŸŒ²")
print("â”€" * 70)
print()

print("ğŸ§  POURQUOI RANDOM FOREST ?")
print()
print("A. La Faiblesse de l'Individu (Arbre de DÃ©cision)")
print("   Un arbre unique â†’ Haute variance â†’ Apprend le bruit")
print()
print("B. La Force du Groupe (Bagging)")
print("   1. Bootstrapping : Chaque arbre voit un Ã©chantillon diffÃ©rent")
print("   2. Feature Randomness : Ã€ chaque nÅ“ud, âˆšn_features alÃ©atoires")
print("   â†’ DiversitÃ© maximale des opinions")
print()
print("C. Le Consensus (Vote)")
print("   100 arbres votent â†’ Les erreurs s'annulent â†’ Le signal Ã©merge")
print()

if problem_type == 'classification':
    model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)
else:
    model = RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10)

print("ğŸ—ï¸  Construction du modÃ¨le...")
print(f"   â€¢ n_estimators=100 (100 arbres)")
print(f"   â€¢ max_depth=10 (profondeur max par arbre)")
print()

print("ğŸš€ EntraÃ®nement en cours...")
model.fit(X_train_scaled, y_train)
print("âœ… ModÃ¨le entraÃ®nÃ© avec succÃ¨s !")
print()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PHASE 8 : Ã‰VALUATION (L'HEURE DE VÃ‰RITÃ‰)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("â”€" * 70)
print("8ï¸âƒ£  AUDIT DE PERFORMANCE : L'HEURE DE VÃ‰RITÃ‰")
print("â”€" * 70)
print()

y_pred = model.predict(X_test_scaled)

if problem_type == 'classification':
    accuracy = accuracy_score(y_test, y_pred)
    print(f"ğŸ¯ ACCURACY GLOBALE : {accuracy*100:.2f}%")
    print()
    
    print("ğŸ“Š RAPPORT DÃ‰TAILLÃ‰ (Classification Report) :")
    print(classification_report(y_test, y_pred, digits=3))
    print()
    
    print("ğŸ” DÃ‰CRYPTAGE DES MÃ‰TRIQUES :")
    print("   â€¢ Precision : QualitÃ© de l'alarme (TP / (TP + FP))")
    print("   â€¢ Recall : Puissance du filet (TP / (TP + FN))")
    print("   â€¢ F1-Score : Moyenne harmonique (2 Ã— Precision Ã— Recall / (P + R))")
    print()
    
    # Matrice de confusion
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar_kws={'label': 'Nombre'})
    plt.title('Matrice de Confusion : RÃ©alitÃ© vs IA', fontsize=14, fontweight='bold')
    plt.ylabel('Vraie Classe', fontsize=12)
    plt.xlabel('Classe PrÃ©dite', fontsize=12)
    plt.tight_layout()
    plt.savefig('confusion_matrix.png', dpi=150, bbox_inches='tight')
    print("ğŸ“ˆ Matrice de confusion sauvegardÃ©e : confusion_matrix.png")
    print()
    
    print("ğŸ“‹ ANALYSE DE LA MATRICE DE CONFUSION :")
    print(f"   â€¢ Vrais Positifs (TP)  : {cm[1, 1]} âœ…")
    print(f"   â€¢ Vrais NÃ©gatifs (TN)  : {cm[0, 0]} âœ…")
    print(f"   â€¢ Faux Positifs (FP)   : {cm[0, 1]} âš ï¸  (Erreur Type I)")
    print(f"   â€¢ Faux NÃ©gatifs (FN)   : {cm[1, 0]} âš ï¸  (Erreur Type II)")
    
else:
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    
    print(f"ğŸ“ MÃ‰TRIQUES DE RÃ‰GRESSION :")
    print(f"   â€¢ RÂ² Score (Coefficient de dÃ©termination) : {r2:.4f}")
    print(f"   â€¢ RMSE (Root Mean Squared Error)          : {rmse:.4f}")
    print(f"   â€¢ MAE (Mean Absolute Error)               : {mae:.4f}")
    print()
    
    print("ğŸ” INTERPRÃ‰TATION :")
    print(f"   â€¢ RÂ² = {r2:.2%} â†’ Le modÃ¨le explique {r2:.1%} de la variance")
    if r2 > 0.7:
        print("     âœ… Excellente performance !")
    elif r2 > 0.5:
        print("     âœ“ Performance acceptable")
    else:
        print("     âš ï¸  Performance Ã  amÃ©liorer")
    print(f"   â€¢ RMSE = {rmse:.2f} â†’ Erreur moyenne de prÃ©diction")
    print()
    
    # Graphique PrÃ©dictions vs RÃ©alitÃ©
    plt.figure(figsize=(10, 5))
    plt.subplot(1, 2, 1)
    plt.scatter(y_test, y_pred, alpha=0.5, color='#4ECDC4', edgecolors='black')
    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 
             'r--', lw=2, label='PrÃ©diction parfaite')
    plt.xlabel('Valeurs RÃ©elles')
    plt.ylabel('Valeurs PrÃ©dites')
    plt.title('PrÃ©dictions vs RÃ©alitÃ©')
    plt.legend()
    plt.grid(alpha=0.3)
    
    plt.subplot(1, 2, 2)
    residuals = y_test - y_pred
    plt.hist(residuals, bins=30, color='#FF6B6B', edgecolor='black', alpha=0.7)
    plt.xlabel('RÃ©sidus (Erreur)')
    plt.ylabel('FrÃ©quence')
    plt.title('Distribution des Erreurs')
    plt.axvline(x=0, color='black', linestyle='--', linewidth=2)
    plt.grid(alpha=0.3)
    
    plt.tight_layout()
    plt.savefig('regression_analysis.png', dpi=150, bbox_inches='tight')
    print("ğŸ“ˆ Graphiques sauvegardÃ©s : regression_analysis.png")

print()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PHASE 9 : IMPORTANCE DES FEATURES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("â”€" * 70)
print("9ï¸âƒ£  INTERPRÃ‰TABILITÃ‰ : QUELLES VARIABLES COMPTENT ?")
print("â”€" * 70)
print()

feature_importance = pd.DataFrame({
    'Feature': X_clean.columns,
    'Importance': model.feature_importances_
}).sort_values('Importance', ascending=False)

print("ğŸ” Top 10 des features les plus importantes :")
print(feature_importance.head(10).to_string(index=False))
print()

# Visualisation
plt.figure(figsize=(10, 6))
top_n = min(15, len(feature_importance))
sns.barplot(data=feature_importance.head(top_n), y='Feature', x='Importance', 
            palette='viridis')
plt.title(f'Top {top_n} Features par Importance', fontsize=14, fontweight='bold')
plt.xlabel('Importance Relative')
plt.tight_layout()
plt.savefig('feature_importance.png', dpi=150, bbox_inches='tight')
print("ğŸ“ˆ Graphique d'importance sauvegardÃ© : feature_importance.png")
print()

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# PHASE 10 : CONCLUSION ET RECOMMANDATIONS
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

print("=" * 70)
print("ğŸ“ CONCLUSION : SYNTHÃˆSE DU PROJET")
print("=" * 70)
print()

print("ğŸ“ CE QUE NOUS AVONS APPRIS :")
print()
print("1. CONTEXTE MÃ‰TIER")
print("   â†’ Comprendre le problÃ¨me avant de coder est crucial")
print("   â†’ Les coÃ»ts d'erreur ne sont pas symÃ©triques en finance")
print()
print("2. DATA WRANGLING")
print("   â†’ Les donnÃ©es rÃ©elles sont toujours sales (NaN, outliers)")
print("   â†’ Attention au Data Leakage lors de l'imputation")
print()
print("3. EDA (EXPLORATION)")
print("   â†’ .describe() rÃ©vÃ¨le distribution et outliers")
print("   â†’ La corrÃ©lation â‰  causalitÃ© (mais aide Ã  dÃ©tecter la redondance)")
print()
print("4. MODÃ‰LISATION")
print("   â†’ Random Forest = robuste, interprÃ©table, peu de tuning")
print("   â†’ Le vote de 100 arbres annule le bruit individuel")
print()
print("5. Ã‰VALUATION")
if problem_type == 'classification':
    print("   â†’ Accuracy seule est trompeuse (classes dÃ©sÃ©quilibrÃ©es)")
    print("   â†’ Recall est critique pour minimiser les faux nÃ©gatifs")
else:
    print("   â†’ RÂ² mesure la qualitÃ© d'ajustement")
    print("   â†’ RMSE donne l'erreur moyenne en unitÃ©s rÃ©elles")
print()

print("ğŸš€ RECOMMANDATIONS POUR ALLER PLUS LOIN :")
print("   â€¢ Tester d'autres algorithmes (XGBoost, LightGBM)")
print("   â€¢ Optimiser les hyperparamÃ¨tres (GridSearchCV)")
print("   â€¢ Feature Engineering : crÃ©er de nouvelles variables")
print("   â€¢ Cross-Validation : valider sur plusieurs folds")
print("   â€¢ DÃ©ploiement : API Flask/FastAPI pour la production")
print()

print("=" * 70)
print("âœ… ANALYSE TERMINÃ‰E AVEC SUCCÃˆS")
print("=" * 70)
print()
print(f"ğŸ“… Date : {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
print("ğŸ‘¨â€ğŸ’» GÃ©nÃ©rÃ© par : Script d'Analyse AutomatisÃ©e")
print()
print("ğŸ“‚ Fichiers gÃ©nÃ©rÃ©s :")
print("   â€¢ eda_analysis.png")
if problem_type == 'classification':
    print("   â€¢ confusion_matrix.png")
else:
    print("   â€¢ regression_analysis.png")
print("   â€¢ feature_importance.png")
print()
print("ğŸ™ Merci d'avoir utilisÃ© ce guide pÃ©dagogique !")
print("="*70)
