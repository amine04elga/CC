import numpy as np, pandas as pd, matplotlib.pyplot as plt, seaborn as sns, os, warnings
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, accuracy_score, classification_report, confusion_matrix
from datetime import datetime
warnings.filterwarnings('ignore')

sns.set_theme(style="whitegrid", palette="husl")
plt.rcParams['figure.figsize'] = (12, 6)
plt.rcParams['font.size'] = 10

rapport = ""
rapport += "# Analyse des tendances de march√© et facteurs externes\n\n"
rapport += f"_Rapport g√©n√©r√© automatiquement le {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}_\n\n"

# PHASE 1 : CONTEXTE
rapport += "## 1Ô∏è‚É£ Contexte m√©tier et objectif\n\n"
rapport += (
    "Dans le monde de la finance et du trading, les d√©cisions d'investissement "
    "reposent sur la compr√©hension des tendances de march√© et des facteurs externes "
    "(√©conomiques, politiques, sociaux).\n\n"
)
rapport += "- **Objectif** : construire un mod√®le pr√©dictif pour anticiper les mouvements de march√©.\n"
rapport += "- **Enjeux** : limiter les pertes (faux positifs) et le manque √† gagner (faux n√©gatifs).\n\n"

# PHASE 2 : ACQUISITION
rapport += "## 2Ô∏è‚É£ Acquisition et chargement des donn√©es\n\n"
rapport += "Les donn√©es proviennent du dataset Kaggle `market-trend-and-external-factors-dataset`.\n\n"
rapport += "```
rapport += "import kagglehub, os, pandas as pd\n"
rapport += "path = kagglehub.dataset_download('kundanbedmutha/market-trend-and-external-factors-dataset')\n"
rapport += "csv_files = [f for f in os.listdir(path) if f.endswith('.csv')]\n"
rapport += "df = pd.read_csv(os.path.join(path, csv_files))\n"
rapport += "```\n\n"

try:
    import kagglehub
    path = kagglehub.dataset_download("kundanbedmutha/market-trend-and-external-factors-dataset")
    csv_files = [f for f in os.listdir(path) if f.endswith('.csv')]
    if csv_files:
        df = pd.read_csv(os.path.join(path, csv_files[0]))
        rapport += f"- Fichier charg√© : **{csv_files[0]}**\n\n"
    else:
        raise FileNotFoundError("Aucun fichier CSV trouv√©")
except Exception as e:
    rapport += f"> ‚ö†Ô∏è Erreur lors du t√©l√©chargement r√©el : `{e}`\n\n"
    rapport += "> Des donn√©es synth√©tiques ont √©t√© g√©n√©r√©es pour la d√©monstration.\n\n"
    np.random.seed(42)
    n_samples = 1000
    df = pd.DataFrame({
        'Date': pd.date_range('2020-01-01', periods=n_samples, freq='D'),
        'Market_Index': np.cumsum(np.random.randn(n_samples) * 2 + 0.05) + 1000,
        'GDP_Growth': np.random.uniform(1.5, 4.5, n_samples),
        'Inflation_Rate': np.random.uniform(1.0, 5.0, n_samples),
        'Interest_Rate': np.random.uniform(0.5, 3.5, n_samples),
        'Unemployment_Rate': np.random.uniform(3.0, 8.0, n_samples),
        'Consumer_Confidence': np.random.uniform(80, 120, n_samples),
        'Oil_Price': np.random.uniform(40, 100, n_samples),
        'Gold_Price': np.random.uniform(1500, 2000, n_samples),
        'USD_Exchange_Rate': np.random.uniform(0.85, 1.15, n_samples),
        'Market_Volatility': np.random.uniform(10, 40, n_samples),
    })
    df['Market_Trend'] = (df['Market_Index'].pct_change() > 0).astype(int)
    df.loc[0, 'Market_Trend'] = 1

rapport += f"- Nombre de lignes : **{df.shape[0]}**\n"
rapport += f"- Nombre de colonnes : **{df.shape[1]}**\n\n"

rapport += "### Aper√ßu des premi√®res lignes\n\n```
rapport += df.head().to_string()
rapport += "\n```\n\n"

rapport += "### Informations sur les types de donn√©es\n\n```
from io import StringIO
buf = StringIO()
df.info(buf=buf)
rapport += buf.getvalue()
rapport += "```\n\n"

# PHASE 4 : DATA WRANGLING
rapport += "## 4Ô∏è‚É£ Data wrangling : nettoyage et pr√©paration\n\n"

df_dirty = df.copy()
numeric_cols = df.select_dtypes(include=[np.number]).columns
for col in numeric_cols[:5]:
    mask = np.random.rand(len(df)) < 0.03
    df_dirty.loc[mask, col] = np.nan

missing = df_dirty.isnull().sum()
missing_nonzero = missing[missing > 0]

rapport += "### Valeurs manquantes introduites (simulation)\n\n```
rapport += missing_nonzero.to_string()
rapport += "\n```\n\n"

if 'Market_Trend' in df_dirty.columns:
    target_col = 'Market_Trend'
    problem_type = 'classification'
elif 'Market_Index' in df_dirty.columns:
    target_col = 'Market_Index'
    problem_type = 'regression'
else:
    numeric_cols = df_dirty.select_dtypes(include=[np.number]).columns
    target_col = numeric_cols[-1]
    problem_type = 'regression'

date_cols = df_dirty.select_dtypes(include=['datetime64', 'object']).columns
X = df_dirty.drop(columns=[target_col] + list(date_cols))
y = df_dirty[target_col]

rapport += f"- Variable cible : **{target_col}**\n"
rapport += f"- Type de probl√®me : **{problem_type.upper()}**\n"
rapport += f"- Nombre de variables explicatives : **{X.shape[1]}**\n\n"

rapport += "### Imputation des valeurs manquantes\n\n"
rapport += "- Strat√©gie : **moyenne (mean)** sur les variables num√©riques.\n\n"

imputer = SimpleImputer(strategy='mean')
X_imputed = imputer.fit_transform(X)
X_clean = pd.DataFrame(X_imputed, columns=X.columns)

rapport += "Apr√®s imputation, il ne reste plus de valeurs manquantes dans les features.\n\n"

rapport += "> ‚ö†Ô∏è Remarque importante : dans ce script p√©dagogique, l'imputation est faite avant le split Train/Test.\n"
rapport += "> En production, il faut **fit** l'imputeur sur le train uniquement pour √©viter le data leakage.\n\n"

# PHASE 5 : EDA
rapport += "## 5Ô∏è‚É£ Analyse exploratoire (EDA)\n\n"
rapport += "### Statistiques descriptives (5 premi√®res variables)\n\n```
rapport += X_clean.iloc[:, :5].describe().round(2).to_string()
rapport += "\n```\n\n"

rapport += "Interpr√©tation rapide de `.describe()` :\n"
rapport += "- **mean vs 50% (m√©diane)** : forte diff√©rence ‚Üí distribution asym√©trique.\n"
rapport += "- **std** : mesure de dispersion (proche de 0 ‚Üí variable peu informative).\n"
rapport += "- **min/max** : utiles pour rep√©rer des valeurs aberrantes.\n\n"

corr_matrix = X_clean.corr()
high_corr = np.where(np.abs(corr_matrix) > 0.9)
high_corr_pairs = [(corr_matrix.index[x], corr_matrix.columns[y], corr_matrix.iloc[x, y])
                   for x, y in zip(*high_corr) if x != y and x < y]

if high_corr_pairs:
    rapport += "### Multicollin√©arit√© (corr√©lations > 0.9)\n\n```
    for var1, var2, corr in high_corr_pairs[:5]:
        rapport += f"{var1} ‚Üî {var2} : {corr:.3f}\n"
    rapport += "```\n\n"
else:
    rapport += "Aucune corr√©lation excessive (>0.9) d√©tect√©e entre les variables num√©riques.\n\n"

plt.figure(figsize=(10, 4))
if problem_type == 'classification':
    y.value_counts().plot(kind='bar', color=['#FF6B6B', '#4ECDC4'])
    plt.title('Distribution de la variable cible')
    plt.xlabel(target_col)
    plt.ylabel('Fr√©quence')
else:
    plt.hist(y, bins=50, color='#4ECDC4', edgecolor='black', alpha=0.7)
    plt.title('Distribution de la variable cible')
    plt.xlabel(target_col)
    plt.ylabel('Fr√©quence')
plt.tight_layout()
plt.savefig('target_distribution.png', dpi=150, bbox_inches='tight')
plt.close()

rapport += "Un graphique de distribution de la cible a √©t√© sauvegard√© sous `target_distribution.png`.\n\n"

# PHASE 6 : SPLIT
rapport += "## 6Ô∏è‚É£ Protocole exp√©rimental : Train/Test split\n\n"

X_train, X_test, y_train, y_test = train_test_split(
    X_clean, y, test_size=0.2, random_state=42
)

rapport += f"- Taille du Train set : **{X_train.shape[0]}** observations (80%)\n"
rapport += f"- Taille du Test set : **{X_test.shape[0]}** observations (20%)\n"
rapport += "- `random_state=42` utilis√© pour la reproductibilit√©.\n\n"

rapport += "### Standardisation des variables\n\n"
rapport += "Les features sont mises √† l'√©chelle (moyenne 0, √©cart-type 1) via `StandardScaler`.\n\n"

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# PHASE 7 : MODELE
rapport += "## 7Ô∏è‚É£ Mod√©lisation : Random Forest\n\n"
rapport += "Le mod√®le utilis√© est une **Random Forest**, robuste et capable de capturer des relations non lin√©aires.\n\n"
rapport += "- Avantages : robustesse, peu de tuning, gestion naturelle des interactions et non-lin√©arit√©s.\n"
rapport += "- Hyperparam√®tres principaux : `n_estimators=100`, `max_depth=10`.\n\n"

if problem_type == 'classification':
    model = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)
else:
    model = RandomForestRegressor(n_estimators=100, random_state=42, max_depth=10)

model.fit(X_train_scaled, y_train)

# PHASE 8 : √âVALUATION
rapport += "## 8Ô∏è‚É£ √âvaluation du mod√®le\n\n"

y_pred = model.predict(X_test_scaled)

if problem_type == 'classification':
    accuracy = accuracy_score(y_test, y_pred)
    rapport += f"- **Accuracy globale** : **{accuracy*100:.2f}%**\n\n"
    rapport += "### Rapport de classification\n\n```
    rapport += classification_report(y_test, y_pred, digits=3)
    rapport += "\n```\n\n"

    cm = confusion_matrix(y_test, y_pred)
    rapport += "### Matrice de confusion\n\n```
    rapport += pd.DataFrame(cm,
                            index=["Vrai 0", "Vrai 1"],
                            columns=["Pr√©dit 0", "Pr√©dit 1"]).to_string()
    rapport += "\n```\n\n"

    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar_kws={'label': 'Nombre'})
    plt.title('Matrice de confusion')
    plt.ylabel('Vraie classe')
    plt.xlabel('Classe pr√©dite')
    plt.tight_layout()
    plt.savefig('confusion_matrix.png', dpi=150, bbox_inches='tight')
    plt.close()

    rapport += "Une matrice de confusion a √©t√© sauvegard√©e sous `confusion_matrix.png`.\n\n"
    rapport += "Interpr√©tation :\n"
    rapport += "- **Faux positifs (FP)** : pr√©dictions de hausse qui n'ont pas lieu.\n"
    rapport += "- **Faux n√©gatifs (FN)** : opportunit√©s manqu√©es (hausse non pr√©dite).\n\n"

else:
    mse = mean_squared_error(y_test, y_pred)
    rmse = np.sqrt(mse)
    mae = mean_absolute_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)

    rapport += "### M√©triques de r√©gression\n\n"
    rapport += f"- **R¬≤** : `{r2:.4f}`\n"
    rapport += f"- **RMSE** : `{rmse:.4f}`\n"
    rapport += f"- **MAE** : `{mae:.4f}`\n\n"

    rapport += "Interpr√©tation :\n"
    rapport += "- R¬≤ mesure la part de variance expliqu√©e par le mod√®le.\n"
    rapport += "- RMSE et MAE quantifient l'erreur moyenne en unit√©s de la cible.\n\n"

    plt.figure(figsize=(10, 5))
    plt.subplot(1, 2, 1)
    plt.scatter(y_test, y_pred, alpha=0.5, color='#4ECDC4', edgecolors='black')
    mn, mx = y_test.min(), y_test.max()
    plt.plot([mn, mx], [mn, mx], 'r--', lw=2, label='Pr√©diction parfaite')
    plt.xlabel('Valeurs r√©elles')
    plt.ylabel('Valeurs pr√©dites')
    plt.title('Pr√©dictions vs R√©alit√©')
    plt.legend()
    plt.grid(alpha=0.3)

    plt.subplot(1, 2, 2)
    residuals = y_test - y_pred
    plt.hist(residuals, bins=30, color='#FF6B6B', edgecolor='black', alpha=0.7)
    plt.xlabel('R√©sidus (erreur)')
    plt.ylabel('Fr√©quence')
    plt.title('Distribution des erreurs')
    plt.axvline(x=0, color='black', linestyle='--', linewidth=2)
    plt.grid(alpha=0.3)

    plt.tight_layout()
    plt.savefig('regression_analysis.png', dpi=150, bbox_inches='tight')
    plt.close()

    rapport += "Des graphiques `regression_analysis.png` ont √©t√© g√©n√©r√©s (pr√©dictions vs r√©alit√©, distribution des r√©sidus).\n\n"

# PHASE 9 : IMPORTANCE DES FEATURES
rapport += "## 9Ô∏è‚É£ Interpr√©tabilit√© : importance des variables\n\n"

feature_importance = pd.DataFrame({
    'Feature': X_clean.columns,
    'Importance': model.feature_importances_
}).sort_values('Importance', ascending=False)

rapport += "### Top 10 des variables les plus importantes\n\n```
rapport += feature_importance.head(10).to_string(index=False)
rapport += "\n```\n\n"

plt.figure(figsize=(10, 6))
top_n = min(15, len(feature_importance))
sns.barplot(data=feature_importance.head(top_n), y='Feature', x='Importance', palette='viridis')
plt.title(f'Top {top_n} features par importance')
plt.xlabel('Importance relative')
plt.tight_layout()
plt.savefig('feature_importance.png', dpi=150, bbox_inches='tight')
plt.close()

rapport += "Un graphique d'importance des variables a √©t√© sauvegard√© sous `feature_importance.png`.\n\n"

# PHASE 10 : CONCLUSION
rapport += "## üîü Conclusion et recommandations\n\n"
rapport += "### Synth√®se du projet\n\n"
rapport += "1. **Contexte m√©tier** :\n"
rapport += "   - Compr√©hension des enjeux de pr√©diction des tendances de march√©.\n"
rapport += "2. **Donn√©es** :\n"
rapport += "   - Jeu de donn√©es multi-facteurs (macro√©conomie, march√©s, devises...).\n"
rapport += "3. **Pr√©paration** :\n"
rapport += "   - Nettoyage, gestion des valeurs manquantes, standardisation des variables.\n"
rapport += "4. **Mod√©lisation** :\n"
rapport += "   - Utilisation d'une Random Forest pour capturer des relations complexes.\n"
rapport += "5. **√âvaluation** :\n"
rapport += "   - Analyse des m√©triques principales et des erreurs.\n\n"

if problem_type == 'classification':
    rapport += "- Pour la **classification**, il est crucial d'√©quilibrer pr√©cision et rappel,\n"
    rapport += "  surtout lorsque les co√ªts des faux positifs et faux n√©gatifs sont asym√©triques.\n\n"
else:
    rapport += "- Pour la **r√©gression**, l'am√©lioration du R¬≤ et la r√©duction du RMSE passent\n"
    rapport += "  par un meilleur feature engineering et √©ventuellement des mod√®les plus puissants.\n\n"

rapport += "### Pistes d'am√©lioration\n\n"
rapport += "- Tester d'autres algorithmes (XGBoost, LightGBM, r√©seaux neuronaux).\n"
rapport += "- Optimiser les hyperparam√®tres (GridSearchCV ou RandomizedSearchCV).\n"
rapport += "- Ajouter ou transformer des variables (feature engineering avanc√©).\n"
rapport += "- Mettre en place une validation crois√©e robuste.\n"
rapport += "- Envisager un d√©ploiement (API Flask/FastAPI, dashboard Streamlit).\n\n"

rapport += "---\n\n"
rapport += "Rapport g√©n√©r√© automatiquement par un script Python d'analyse de donn√©es.\n"

with open("rapport_marche.md", "w", encoding="utf-8") as f:
    f.write(rapport)

print("‚úÖ Rapport Markdown g√©n√©r√© : rapport_marche.md")
print("   √Ä d√©poser tel quel sur GitHub avec les images PNG g√©n√©r√©es.")
